---
layout: standard-page
title: Data Quality
slug: data-quality
group: Data Quality
left-content-nav-order: 10

left-nav:
- Data Quality Planning
- Data Profiling
- Data Quality Assessment
- Data Cleansing and Improvement
---

{% include left-content-nav.html %}



<div class="pure-u-14-24 center-content">

	<p class="page-definition">Best practices for planning and implementing a comprehensive approach for the detection of defects, the definition of quality requirements, and the data cleansing and improvements that ensure the approach’s fitness for the data’s intended uses in operations, decision making, and analytics.</p> 

	<p>The Data Quality category process areas comprise an organization-wide approach that enables staff to fully understand the current quality of data under management and institute processes and mechanisms to achieve the following objectives:</p>
	<ul>
		<li>Develop an organization-wide plan to prioritize and plan quality efforts;</li>
		<li>Evaluate, prevent, and remediate data defects;</li>
		<li>Ensure that the quality of data meets business purposes and supports the organization’s tactical and strategic objectives;</li>
		<li>Implement effective processes, procedures, and mechanisms for cleansing data; and</li>
		<li>Undertake business process improvements according to their impact on data capture, management, and quality.</li>
	</ul>

	<p>In short, these process areas describe a comprehensive data quality program, driven by a data quality plan.  Efforts to improve data quality are often scattered and addressed on a project by project basis.  For a data set as vital and central as patient demographic data, all stakeholders who create, update, or modify the data through any business process must be involved and accept shared responsibilities for data quality. This can be summarized as evolving toward a ‘quality culture.’</p>

	<p>Data Quality Planning is foundational for all data quality management activities. It addresses activities to help an organization create a defined, approved, and integrated plan to achieve improved data quality that meets business needs. Data Profiling is the discovery of the actual condition of physical data which reveals defects, anomalies, and opportunities to enhance quality rules.  Data Quality Assessment addresses activities that help the organization evaluate critical data sets against defined quality objectives. Data Cleansing and Improvement assists the organization to develop successful and repeatable processes for cleansing data, supports root cause analysis, and reveals opportunities to enhance business processes to produce better quality data.</p>

	<p>Implementing the practices contained in this category will help to translate quality objectives and priorities into actionable plans, designed to be administered as an organization-wide program to actively manage patient demographic data across all of the dimensions of quality. The organization can then recognize and capitalize on opportunities that allow it to realize maximum value from accurate, trusted data.</p>


	<h3 id="data-quality-planning" class="subheading">Data Quality Planning</h3>

	<h4>Purpose</h4>  

	<p>Defines an integrated plan to improve and maintain the level of data quality needed to accomplish business goals and objectives.</p>

	<h4>Introductory Notes</h4> 

	<p>Data quality planning is the process of defining the business goals, objectives, specfic initiatives, and sustained activities to improve data integrity, accuracy, and trustworthiness. The data quality plan, providing an approved organization-wide approach, becomes a unifying force to foster shared responsibility for quality.</p>

	<p>Organizations typically approach data quality improvement project by project, data store by data store.  However, without an overall organization-wide focus on the business criticality of high quality data, positive results are frequently isolated in disconnected pockets.  If processes, methods, and procedures are developed independently for each effort, the organization risks:  lack of awareness on the part of business staff about quality needs across the data lifecycle; undue effort and duplicate costs; and uncoordinated inefficient implementations (for example, repeatedly cleansing data in a downstream data store while not improving data quality at the source).</p>

	<p>The data quality plan has a simple purpose – support focused, strategic thinking.  By engaging in organization-wide collaborative considerations of basic questions, issues and objectives will be surfaced and agreement can be forged. The organization should engage stakeholders from all relevant business areas to pose these questions about patient demographic data:</p>
	<ul>
		<li>What are our biggest quality issues in each data store containing patient demographic data?</li>
		<li>What goals and objectives will we achieve if quality is improved?</li>
		<li>What are the impacts of poor data quality, e.g., cost, risk, compliance, productivity?</li>
		<li>How will we monitor and sustain our activies to ensure that we meet our goals?</li>
		<li>How will we prioritize our efforts? And how will we decide where to direct our time and effort, such as cleaning up duplicates, working with vendors to improve capabilities, and training patient registration staff?</li>
		<li>What is the proposed sequence to accomplish our objectives?</li>
		<li>How should the organization engage to review objectives, direct resources, and address business process and technical issues?</li>
		<li>Who should be involved, who will lead and coordinate improvement efforts, and how will we structure their engagement?</li>
		<li>How will data quality improvements be measured, analyzed, and reported on over time?</li>
	</ul>

	<p>In a large organization, this planning effort may extend over a few weeks.  In a small organization, it may be completed in a few brief meetings.  Once the approach and corresponding specific initiatives are approved (See Data Governance) the data quality plan can be finalized.  It should address:</p>
	<ul>
	  	<li>Quality goals and objectives (e.g, specific objectives such as minimizing duplicates to under 4% in year one, 3% in year two, etc.);</li>
	  	<li>Quality principles (e.g., capture and update patient demographic data in an accurate and timely manner);</li>
	  	<li>Major issues that the plan commits the organization to address (e.g., commitment to reduce patient safety events due to misidentification by 50% in year one);</li>
	  	<li>Anticipated benefits (e.g., what improved outcomes are expected);</li>
	  	<li>Responsibilities and accountability – (e.g., what roles are needed to encourage coordination across departments, and how engagement should be evaluated and modified as needed, etc.);</li>
	  	<li>Toolsets – (e.g., the scope of the issues justifies selecting and implementing an in-house data quality tool, or the organization may choose to request vendor services, etc.);</li>
	  	<li>Sequence plan of initiatives – (e.g., the first initiative will be a data profiling pilot project; the next iniative will be training for all staff; next will be establishing a data quality assessment process with business representatives, etc.)</li>
	  	<li>Training – the organization should plan to conduct training for every individual who enters, updates, or modifies data (e.g., during a record merge).</li>
	  </ul>   

	<p>Although technologies, methods, and specific techniques can be required, in the end high quality data is the result of continued attention by all relevant stakeholders, communicated and shared across the organization. Creating and following the data quality plan is a positive cultural shift, demonstrating the organization’s executive commitment, and an intention to educate, engage, and sustain effective and direct attention focused on patient demographic data. It is essential to conduct training for everyone who enters, updates, or modifies data (e.g., during a records merge).  Training topics should include: process workflow, current system capabilities, privacy and security/HIPAA, and how feedback should be submitted for suggested improvements.</p>

	<p>The data quality plan establishes and is implemented by the key quality processes.  (See Data Profiling, Data Quality Assessment, and Data Cleansing and Improvement.)  Following the data quality plan will assist the organization to realize these benefits:</p>
	<ul>
		<li>Clear, approved path to improved business operations engendering stakeholder involvement and providing confidence that data quality issues will be addressed and prevented in the future;</li>
		<li>Transformed ad hoc efforts to planned quality progress, saving effort and costs;</li>
		<li>Organization-wide approach facilitating funding requests for projects and resources;</li>
		<li>Defined responsibilities parsing out clear roles, from the boardroom to the database administrator;</li>
		<li>Approved prioritization, consensus, and enhanced awareness among business staff, creating a ‘quality culture’; and</li>
		<li>Reusability of approaches, methods, toolsets, techniques, and procedures.</li>
	</ul>
	

	<h3 id="data-profiling" class="subheading">Data Profiling</h3>

	<h4>Purpose</h4>  

	<p>Develop and apply discovery techniques to physical data sets to develop an understanding of the content, quality, and rules of a specified set of data under management.</p>

	<h4>Introductory Notes</h4> 

	<p>Data profiling is a cornerstone of an effective data quality improvement effort and is an important first step for many information technology initiatives. It is a discovery task conducted through automated (tool supported or custom queries) and/or manual analysis of physical records. For a selected data set, it reveals what is stored in databases and how physical values may differ from expected, allowed, or required values listed in data store documentation or described in metadata repositories.  Definite errors are referred to as “defects.” Suspected errors are referred to as “anomalies.”</p>

	<p>Data profiling differs from Data Quality Assessment in that profiling activities result in a series of conclusions about physical data sets, whereas the assessment process evaluates how well the data meets specific business quality requirements. Data profiling is typically the first step in conducting data quality assessments.</p>

	<p>There are several levels of tests a data profiler can apply to a data set.  At the most basic level, vendor data quality tools contain out-of-the-box tests that examine nulls, lengths, ranges, values, and formats.  As a hypothetical example, if a profiling effort were conducted on 1,000 patient demographic records, the basic test (aka, “syntax” checks) results might yield defects and anomalies similar to these: 13 birth dates were entered in a MMDDYY format instead of the intended MMDDYYYY format; 27 street addresses contained less than 7 characters; 34 records did not contain a phone number; 15 records did not contain a ZIP code; 193 records did not contain a middle name; 3 records contained a patient gender value of ‘O’.</p>

	<p>Another level of profiling includes tests on values that affect each other within a given record according to business rules (aka, “semantic” checks).  For example, checking if a record with a patient status of “Deceased” also contains a date value in the date of death column.  More complex exploration can also be applied between different records (aka, “context” tests), for example, determining that two patients are co-located (aka, “householding”) based on the occurrence of identical addresses, or identifying duplicate patient records. This level of profiling typically requires custom queries.</p>

	<p>Profiling results are a primary input to resolution of anomalies, typically through manual analysis and data cleansing activities.  In the example above of unusually short street addresses, a manual review may find that the values are valid, e.g., Zip ST, Fen ST, etc.;  in the gender value example, the reviewer may need to research the gender of the three patients; in the case of missing ZIP codes or phone numbers, the reviewer might refer the lookup to billing.  Profiling results also feed into data cleansing to correct errors, for example, standardization of the MMDDYYYY format can be implemented through an automated script.</p>

	<p>An organization that implements effective profiling practices can realize the following benefits:</p> 
	<ul>
		<li>Deepen understanding of patient demographic data among all stakeholders and fosters a quality culture;</li>
		<li>Increase reliability of data for research, population health and other quality reporting analytics, accuracy of Meaningful Use (MU) reporting, and interoperability;</li>
		<li>Identify and correct critical defects;</li>
		<li>Reliable discovery of anomalies and duplicates;</li>
		<li>Ability to update and adjust profile as the patient population changes over time and as new standards are enacted;</li>
		<li>Increase savings in effort and cost through reuse of quality rules; and</li>
		<li>Discovery of enhancements to business processes, technologies, and procedures to prevent errors.</li>
	</ul>

	<p>It is recommended that an organization which hosts at least one data store containing patient demographic data first conduct a comprehensive baseline profiling effort. This will clearly reveal the current state of the data, allow formulation of meaningful metrics, and become the foundation for tracking improvements going forward.</p>

	<p>Once the data has undergone baseline cleansing and anomaly resolution, the organization should determine the frequency at which it will conduct periodic profiling of the patient demographic data, reusing the same tests and enhancing with new tests if additional quality rules are adopted through data quality assessments, changes in standards, or additions to demographic data.  For example, adopting and following a standard practice of basic profiling and cleansing before running advanced rule sets (i.e. algorithms) for identifying matching patient records will help to increase the accuracy of matching algorithms.  This may initially increase the number of duplicates discovered (e.g., false negatives), but should lower the instance of duplicate records over time.</p>

	<p>Data profiling can also be event-driven as an important activity in evaluating data quality for a specific purpose. For example, if a healthcare organization acquires another organization, profiling the new source would be recommended prior to migrating and integrating records into the destination system.  Similarly, organizations are advised to profile data prior to: finalizing the design of a master data store; performing data conversion or consolidation; implementing a new EHR, patient registration, billing, or similar systems or migration; and connecting to a health information exchange.</p>

	<p>In addition to defect fixes and resolution of anomalies, organizations may determine that results indicate that: metadata descriptions should be updated, loading scripts should be enhanced with refined or new quality rules, or existing data structures should be redesigned.  What is learned through data profiling often also serves as a key input to development of metrics, content standards, business process redesign, and technology refreshes, to further improve the quality of the organization's patient data.</p>

	<p>It is recommended that an organization implement a standard process for conducting profiling efforts, which includes the reporting and publication of results to relevant stakeholders. Prioritization of candidate data stores and data sets for profiling should be based on business needs and objectives expressed in the data quality plan.</p>

	
	<h3 id="data-quality-assessment" class="subheading">Data Quality Assessment</h3>
	
	<h4>Purpose</h4>  

	<p>Provides a systematic, business-driven approach to measure and evaluate data quality employing data quality dimensions, to ensure fitness for purpose and establish targets and thresholds for quality.</p>

	<h4>Introductory Notes</h4> 

	<p>The business owns the data it creates and manages.  No organization’s information technology staff can single-handedly improve the quality of its data.  Business representatives across the patient demographic lifecycle must be engaged to: determine patient demographic data’s fitness for purpose across the lifecycle; define the level of quality desired; and define the level of quality acceptable.</p>

	<p>The data quality assessment processes consist of making decisions about the data and acting on those decisions. Only those who create, modify, and delete patient demographic data, across every phase of its lifecycle, can decide:</p>
	<ul>
		<li>If the data set is sufficiently complete and accurate to support business process needs (i.e., “fit for purpose”);</li>
		<li>What is the desired state of specific attributes (i.e., “targets”);</li>
		<li>What is the minimum level of quality acceptable (i.e., “thresholds”);</li>
		<li>What are the best measures and metrics to track improvements.</li>
	</ul>

	<p>To take an example of fitness for purpose, an organization may discover, based on data profiling results, that it does not capture a sufficient set of attributes to maximize the efficacy of its record matching algorithm.  The data set is not fit for purpose because it is incomplete with respect to the business objective of preventing and reconciling duplicates. Business representatives would need to consider which attributes are most useful to add to constitute the minimum set that is required, for instance, mother’s maiden name, previous address, previous phone number, etc.</p>

	<p>An example of targets might be: 100% population of all attributes (each specified) needed for matching. An example of a threshold might be: 95% of first names must contain more than one character (the rationale here might be that later in the patient lifecycle this could be modified).  An example of a metric might be: the baseline profiling effort revealed that 10% of the street addresses did not have a street suffix (RD, ST, BLVD, etc.).  This metric, percentage of records without street suffixes, could be monitored to assess improvement as data profiling monitoring and data cleansing activities were conducted over time.</p>

	<p>The most effective mechanism to assist the business in assessing data quality and establishing useful targets, thresholds, and metrics is the consideration and application of data quality dimensions to each attribute. A “dimension” is a criterion against which data quality is measured. A number of different dimensions of quality can be measured. A sample set often used is presented below:</p>
	<ul>
		<li>Accessibility – the data is available when needed;</li>
		<li>Accuracy – affinity with original intent, veracity as compared to an authoritative source, correlation of data elements (e.g., an insurance card and a driver’s license), and measurement precision (e.g., a patient’s last name is correctly spelled);</li>
		<li>Completeness – availability of required data attributes (e.g., a ZIP code is missing);</li>
		<li>Coverage – availability of required data records (e.g., some returning patient’s records are missing);</li>
		<li>Conformity – alignment of content with required standards (e.g. birth date formatted as MMDDYYYY);</li>
		<li>Consistency – compliance with required patterns and uniformity rules (e.g., “Street” must be abbreviated to “ST”), supported by data entry standards, workflow management, and technical design standards;</li>
		<li>Integrity – accuracy of data relationships (parent and child linkage, e.g., patient is correctly represented as the mother of another patient);</li>
		<li>Timeliness – the currency of content (e.g., the patient’s name change is recorded as soon as it is known, and automatically updated across all relevant data stores); and</li>
		<li>Uniqueness – each record can be unambiguously identified (e.g., patient lookup and other reports provide a unique ID, versus Last Name, First Name, and Date of Birth) – uniqueness also includes checks for redundancy of records (e.g., duplicate patient records).</li>
	</ul>

	<p>Performing a data quality assessment is based on the predefined quality expectations and criteria set by stakeholders and approved by governance.  It is advisable to start by measuring data quality for a small set of key attributes supporting one or more primary business processes, i.e., patient demographic data. Profiling the data is the recommended first step. For each attribute identified, the organization should convene a working group (e.g., data stewards) representing all relevant stakeholders to determine targets, set thresholds, and define the quality dimensions that are most important.</p>

	<p>Once the criteria are determined and the data evaluated, metrics can be developed and published in a scorecard or dashboard format. Assessment results facilitate root cause analysis and are key inputs into the organization's data quality improvement plans. Periodic assessments should be conducted to determine if acceptable thresholds and targets are being met, and metrics should be updated accordingly.</p>

	<p>To support these efforts and track improvements over time, it is helpful to conduct an impact analysis of the overall data quality effort, as well as specific impacts of improvements regarding individual data elements, as part of the assessment process. Categorizing impacts of poor data quality, such as cost, risk, compliance, productivity, etc. also assists in prioritizing data cleansing and quality improvement plans.</p>

	<p>Effective goverance is important to implementing this process (See Data Governance).  Assignment of specific responsibilities and data ownership deepens business engagement, which is important because improving data quality is truly a team effort. For example, an organization may decide that the Billing department should own ZIP code because it is critical for mailing patient bills; whereas, it is not critical for clinical care delivery. Under the supervision of the data quality coordinator, if ZIP codes were found to be missing or inaccurate, Billing would initiate root cause analysis and sponsor the resulting improvements for remediation and defect prevention. The data quality assessment process and accompanying mechanisms and metrics provide the following benefits:</p>
	<ul>
		<li>Anchors responsibility in the departments for the quality of their data;</li>
		<li>Results in tangible improvements for each line of business;</li>
		<li>Deepens stakeholder knowledge of the data and refines quality rules;</li>
		<li>Creates a “quality culture” via a collaborative effort sustained over time;</li>
		<li>Proves that the organization’s data assets are becoming more trustworthy;</li>
		<li>Supports realistic cost estimates and better planning with an impact analysis; and</li>
		<li>Published data quality metrics:
			<ul>
				<li>Inform internal and external consumers;</li>
				<li>Improve population health analytics, quality reporting, and other reporting such as Meaningful Use; and</li>
				<li>Foster interoperability.</li>
			</ul>
		</li>
	</ul>

	<h3 id="data-cleansing-and-improvement" class="subheading">Data Cleansing and Improvement</h3>
	
	<h4>Purpose</h4>  

	<p>Addresses the mechanisms, processes, and methods used to validate and correct data defects according to predefined quality rules, as well as analysis and enhancement of business processes to prevent errors.</p>

	<h4>Introductory Notes</h4> 

	<p>Data cleansing focuses on data correction to meet business user criteria (targets and thresholds) as determined by data quality rules addressing all applicable quality dimensions. Quality rules, developed through the data quality assessment process and the results of data profiling efforts, provide a baseline for identifying data defects which can affect business operations.</p>

	<p>An example of enhancing a business process to improve patient demographic data quality might be, for example: once a patient has produced identification, if there is an existing record in the system, providing the patient a subset of the demographic information for validation. Adding this activity step would help prevent potential duplications.</p>

	<p>Data cleansing activities are most effective when conducted at, or as close as possible to, the point of first capture, i.e. the first automated data store to record the patient’s data, or as close to the original creation point as feasible. A best practice is to undertake cleansing activites based on data profiling or data quality assessment analysis. The organization should develop a standard data cleansing plan template to ensure that cleansing rules for data are shared and reused for any data store in which patient demographic data is located.  This will help to minimize duplication of effort and conflicting cleansing activities, e.g. cleaning the same data in two physical locations but applying different rules.</p>

	<p>Organizations need to establish criteria for what events trigger data cleansing efforts. In most organizations, data cleansing is more frequently conducted on key shared data stores, such as a master patient index or the patient registration or scheduling system.  However, it is advisable to expand the scope to operational systems which provide data to other internal or external consumers, subject to criticality and budget. As with data profiling efforts, it is recommended that data cleansing criteria are subject to impact analysis.</p>

	<p>Once accomplished, data corrections should be published and immediately made available for affected downstream data stores and repositories. It is advised to develop and document a consistent process for escalating issues approporiately to governance, the quality coordinator, or the vendor if applicable.  Data changes should be verified with internal and external data providers, preferably through an automated message or report. If this capability does not exist, then a manual report should be produced, and provided both to the designated data quality resource and any corresponding internal or external stakeholders.</p>

	<p>The benefits of implementing standard data cleansing and improvement processes include:</p>
	<ul>
		<li>Realized savings in both effort and cost;</li>
		<li>Errors traced to root causes to prevent downstream errors;</li>
		<li>Increased resource allocation efficiency through cumulative experience;</li>
		<li>Improved and sustainable data quality through discovery of quality rules; and</li>
		<li>Improved effectiveness of business processes to minimize errors.</li>
	</ul> 

</div> <!-- end center content -->


{% include right-content.html %}

